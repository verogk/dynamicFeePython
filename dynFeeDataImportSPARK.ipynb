{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Fee: Getting Data with Spark\n",
    "Se bajan las acciones de los visitantes del sitio en las últimas 4 semanas para un site para el producto hoteles. Nos quedamos con los flows search, detail, checkout y thanks. \n",
    "\n",
    "El script para bajar los eventos esta separado para poder hacer el resto del análisis a la vez, si se tiene otro set de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import wasabisql\n",
    "import pytz\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import udf, lit\n",
    "from math import radians, cos, sin, asin, sqrt, atan2\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.applicationId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importo 4 semanas de datos\n",
    "- En start y end ponemos la fecha de inicio y fin del periodo del que queremos bajar los eventos. \n",
    "- generateDays genera una serie de sub starts y ends de un dia de duracion entre esas fechas.\n",
    "- getEvents baja un dataframe por cada día de eventos y lo filtra.\n",
    "- Cada dataframe se appendea directamente a un archivo parquet que se va sobreescribiendo, sin necesidad de hacer un checkpoint o de ir acumulando los dataframes en una lista.\n",
    " \n",
    "EL proceso se hace de a un día porque a Spark le cuesta trabajar con tantos archivos avro en los que está distribuida la data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loader = wasabisql.SOTLoader(sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "campos = ['userid', 'datetime', 'pr', 'fl', 'cc', 'ci', 'co', 'dc', 'hr', 'hid', 'di','hc', 'pri', \n",
    "                               'pritax', 'cur', 'exch', 'xClient', 'dtype', 'dbr']  \n",
    "campos_sin_pr = ['userid', 'datetime','fl', 'cc', 'ci', 'co', 'dc', 'hr', 'hid', 'di','hc', 'pri', \n",
    "        \n",
    "                 'pritax', 'cur', 'exch', 'xClient', 'dtype', 'dbr']\n",
    "\n",
    "site = \"CL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateDays(start, end):\n",
    "    one_day = datetime.timedelta(1)\n",
    "    i = 0\n",
    "    while start + one_day*i < end:\n",
    "        yield start + one_day*i\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getEventsDay(start, end):\n",
    "\n",
    "    events = loader.getEvents(campos, start, end)\n",
    "\n",
    "    events = events.filter(\"length(userid) = 36\") \\\\\n",
    "        .filter('lower(fl) in (\"search\", \"detail\", \"checkout\", \"thanks\")') \\\\                  \n",
    "        .filter('lower(pr) in (\"hotels\")') \\\\\n",
    "        .filter(\"dc is not null\") \\\\       \n",
    "        .filter('lower(cc) not in (\"cl\", \"ar\", \"br\", \"mx\")')\n",
    "                    \n",
    "    #saco la columna de pr, ya que solo me quede con hoteles. Sacaria cc pero la uso mas adelante\n",
    "    events = events[campos_sin_pr]\n",
    "    \n",
    "    # Ahora el checkpoint se hace con cada events diario\n",
    "    #events = checkpoint(events)\n",
    "    #events.count()\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mode = 'overwrite'\n",
    "for start in generateDays(datetime.datetime(2016,6,28), datetime.datetime(2016,7,25)):\n",
    "    end = start + datetime.timedelta(1)\n",
    "    events = getEventsDay(start, end)\n",
    "    events.write.parquet('dataset/DynFeeEvents-OTHERS-2016-06-27-2016-07-25.parquet', mode=mode)\n",
    "    mode = 'append'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uno todas las semanas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# allevents es una lista de DataFrames\n",
    "events = reduce(DataFrame.unionAll, [allevents])\n",
    "# events es un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events.write.option(\"header\", \"true\").save('dataset/DynFeeEvents-BR-2016-07-06-2016-07-17.parquet', format=\"parquet\", mode=\"Overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
