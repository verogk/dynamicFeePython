{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Fee: Processing Data con Spark\n",
    "Primera parte del procesamiento de datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import wasabisql\n",
    "import pytz\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import udf, lit\n",
    "from math import radians, cos, sin, asin, sqrt, atan2\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.applicationId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levantamos el dataframe de eventos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# para levantar\n",
    "events = sqlContext.read.load('dataset/DynFeeEvents-BR-1WeekTo25jul.parquet', format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agrupar por usuarios\n",
    " \n",
    "Agrupar hace que la lista events se separe en dos partes: una es la key, el userid, y la otra parte son las caraterísticas de las acciones. Más adelante el haber separado por usuario va a hacer que los procesos y funciones como el sorting se hagan por usuario. (para tener en cuenta)\n",
    " \n",
    "(!) CLAVE: los distintos eventos están guardados en diferentes máquinas. Al agrupar por usuario, si eso es relevante para las funciones que voy a aplicar de ahí en adelante, hace que todos los procesos sean más eficientes porque hace que todas las acciones de un mismo usuario se re-ubiquen en una misma máquina!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = events.rdd.groupBy(lambda x: x.userid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtrar a los usuarios compradores\n",
    "#### Esta función marca con True a los userids que tienen al menos una compra. Esto no es un dataframe sino una lista / diccionario. Entonces le estoy pidiendo usuario por usuario que recorra su lista de características y busque si la palabra \"thanks\" aparece al menos una vez dentro de la sub-lista de flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tieneThanks(pair):\n",
    "    #Filter no sabe de pairs.\n",
    "    evs = pair[1]\n",
    "    for event in evs:\n",
    "        if event.fl.lower() == 'thanks':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# pero esto al final!!\n",
    "# Luego filtro y separo a los que dan True como resultado de la funcion TieneThanks.\n",
    "buyers = users.filter(tieneThanks)\n",
    "checkers = users.filter(lambda x: not tieneThanks(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para los compradores, me quedo solo con las acciones previas al primer thanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filterPreThanks(evs):\n",
    "    # obtengo el minimo datetime de los thanks, el mas viejo\n",
    "    min_datetime = min( [ ev.datetime for ev in evs if ev.fl.lower() == 'thanks'] )\n",
    "    # todos los eventos anteriores a min_datetime \n",
    "    result = [ev for ev in evs if ev.datetime <= min_datetime]\n",
    "    return result\n",
    "#todos los eventos antes del thanks y el primer thanks\n",
    "buyers_first_thanks = buyers.mapValues(filterPreThanks)\n",
    "\n",
    "#buyers_first_thanks.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ya puedo volver a unir buyers y checkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allusers = buyers_first_thanks.union(checkers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eventos ordenados por usuario por timestamp\n",
    "##### Para ordenar convierto a diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toDict(evs):\n",
    "    return [ev.asDict() for ev in evs]\n",
    "\n",
    "def sortEvs(evs):\n",
    "    return sorted(evs, key=lambda x: x['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allusers = allusers.mapValues(toDict).mapValues(sortEvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voy a separar el análisis en 2 partes\n",
    "#### 1) POR CARACT: DE NAVEGACION: Contadores de sesión, acciones por flow, dias de navegacion\n",
    "#### 2) POR CARACT DEL VIAJE: Características del viaje del flow más profundo más reciente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) POR NAVEGACION: Contadores de sesión, acciones por flow, dias de navegacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agrego la diferencia de tiempo entre sesiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def agregarDiff(evs):\n",
    "    new_evs = []\n",
    "    evs = list(evs) # convierto a evs, el input de la funcion, en lista, porque la parte Values del RDD es iterable pero no indexable\n",
    "    prev = evs[0]['datetime'] # ahora que converti en lista, puedo indexar evs[0]\n",
    "    for event in evs:\n",
    "        diff = event['datetime'] - prev\n",
    "        prev = event['datetime']\n",
    "        event['diff'] = diff  \n",
    "        new_evs.append(event) #los nuevos eventos los fui acumulando en la variable new_events\n",
    "    return new_evs\n",
    "\n",
    "allusers_con_diff = allusers.mapValues(agregarDiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #### Agrego contador de sesiones\n",
    "# #### la sesion cambia con 30 minutos de inactividad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_session(new_evs):\n",
    "    current_session = 1\n",
    "    for ev in new_evs:\n",
    "        if ev['diff'] > 30*60*1000:\n",
    "            current_session = current_session + 1\n",
    "        ev['session'] = current_session\n",
    "    return new_evs\n",
    "\n",
    "allusers_con_session = allusers_con_diff.mapValues(add_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### Quiero saber la cantidad de sesiones, de dias de sesiones distintas y de acciones antes de una compra\n",
    "# - 1) Me quedo con el maximo del numero de sesion por usuario\n",
    "# - 2) agrego el timestamp de la primera accion de la sesion. Busco el dia al que corresponde y cuento cuantos dias distintos busco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #### Agrego el día del año porque así veo cuantos días distintos dedicó a navegar el sitio hasta comprar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #### Funcion que pasa datetime de GMT a horario local\n",
    "# (dependiendo del pais del evento) Si el input es un timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tstampAsDatetime(tstamp, cc):\n",
    "    cc = cc.upper() # paso a mayuscula el pais porque despues uso ese codigo para buscar el timezone en una libreria\n",
    "    fecha = datetime.datetime.fromtimestamp(tstamp/1000) # paso datetime a fecha. esta funcion toma el datetime sin los decimales\n",
    "    tzinfoMI = pytz.timezone('America/New_York') # fecha es \"naive\". Desconoce su timezone. Busco el TZ de Miami/NY aca\n",
    "    fechaconsciente = tzinfoMI.localize(fecha) # hago que fecha sea consciente de tu timezone\n",
    "    tzname = pytz.country_timezones[cc][0] # country_timezones es una libreria de nombres de timezones.\n",
    "    tzcode = pytz.timezone(tzname) # Vero agrego esto. CHEQUEAR\n",
    "    \n",
    "    # la funcion me devuelve la fecha del evento en horario local del evento, al aplicar el timezone local. \n",
    "    return fechaconsciente.astimezone(tzcode) # Vero cambio tzname por tzcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addLocalDayOfYear(new_evs):\n",
    "    for ev in new_evs:\n",
    "        ev['local_day_of_year'] = tstampAsDatetime(ev['datetime'], ev['cc']).strftime('%j') #strftime('%j'): fecha a n° de dia en el año\n",
    "    # la funcion me devuelve la fecha del evento en horario local del evento, al aplicar el timezone local. \n",
    "    return new_evs\n",
    "\n",
    "allusers_con_localday = allusers_con_session.mapValues(addLocalDayOfYear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #### Sort por datetime de nuevo por las dudas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allusers_condia_sort = allusers_con_localday.mapValues(sortEvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def agregarDayNumber(evs):\n",
    "    new_evs = []\n",
    "    #local_day = new_evs['local_day_of_year']\n",
    "    day_number = 1\n",
    "    prev = evs[0]['local_day_of_year'] # ahora que converti en lista, puedo indexar evs[0]\n",
    "\n",
    "    for event in evs:\n",
    "        if event['local_day_of_year'] > prev :\n",
    "            day_number = day_number + 1\n",
    "            \n",
    "        event['planning_day_number'] = day_number\n",
    "        prev = event['local_day_of_year']\n",
    "        #new_evs = new_evs + [event]\n",
    "        new_evs.append(event)\n",
    "\n",
    "    return new_evs\n",
    "\n",
    "allusers_con_planning_day = allusers_condia_sort.mapValues(agregarDayNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #### Paso a fecha el datetime de la accion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addActionDate(new_evs):\n",
    "    for ev in new_evs:\n",
    "        ev['action_date'] = tstampAsDatetime(ev['datetime'], ev['cc']).strftime(\"%Y-%m-%d\")  #strftime: fecha a n° de dia en el año\n",
    "    # la funcion me devuelve la fecha del evento en horario local del evento, al aplicar el timezone local. \n",
    "    return new_evs\n",
    "\n",
    "allusers_con_actiondate = allusers_con_planning_day.mapValues(addActionDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### Funcion que me dice el numero de sesiones que hizo un usuario y me dice cuantas acciones de cada tipo hizo\n",
    "# #### Para compradores, son las acciones hasta el primer thanks. Para no compradores, son todas las acciones dentro del periodo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxSession(evs):\n",
    "    session_numbers = [event['session'] for event in evs]\n",
    "    return max(session_numbers)\n",
    "\n",
    "def maxPlanningDay(evs):\n",
    "    planning_days = [event['planning_day_number'] for event in evs]\n",
    "    return max(planning_days)\n",
    "\n",
    "def getFeatures(pair):\n",
    "    userid = pair[0]\n",
    "    evs = pair[1]\n",
    "    \n",
    "    result = {}\n",
    "    result['search'] = 0\n",
    "    result['detail'] = 0\n",
    "    result['checkout'] = 0\n",
    "    result['thanks'] = 0\n",
    "    \n",
    "    for ev in evs:\n",
    "        result[ev['fl']] = result[ev['fl']] + 1\n",
    "    \n",
    "    result['max_session'] = maxSession(evs)\n",
    "    result['max_planning_days'] = maxPlanningDay(evs)\n",
    "    result['cant_actions'] = len(evs)\n",
    "    result['userid'] = userid\n",
    "    compras = [event for event in evs if event['fl'] == 'thanks']\n",
    "    comprador = len(compras) > 0\n",
    "    result['comprador'] = comprador\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Luego filtro y separo a los que dan True como resultado de la funcion TieneThanks.\n",
    "allusers_maxsession = allusers_con_planning_day.map(getFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ##### Paso esta tabla a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# El problema para pasar de rdd a dataframe con createDataFrame es que inferschema infiere el schema del primer dict, \n",
    "# pero no todos los dict tienen el mismo schema (porque no todos los eventos tienen los mismos campos)\n",
    "\n",
    "#sqlContext.createDataFrame(allusers_maxsession).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hago yo el schema, asumiendo que son todos strings\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DoubleType, DateType                            ,TimestampType,LongType, BooleanType\n",
    "\n",
    "def inferirSchema(campos):\n",
    "    schema_campo = []\n",
    "    for k in campos:\n",
    "        if k in ['cant_actions', 'max_planning_days', 'max_session', 'checkout', 'detail','thanks', 'search']:\n",
    "            schema_campo.append(StructField(k,  IntegerType(), True))\n",
    "        elif k in ['comprador']:\n",
    "            schema_campo.append(StructField(k, BooleanType(), True))\n",
    "        elif k in ['datetime', 'diff', 'flow_depth', 'planning_day_number', 'session']:\n",
    "            schema_campo.append(StructField(k, LongType(), True))\n",
    "        else:\n",
    "            schema_campo.append(StructField(k, StringType(), True))\n",
    "    return StructType(schema_campo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "campos_agregados = ['userid' ,'cant_actions', 'max_planning_days', 'max_session','search', 'detail','checkout', 'thanks', 'comprador']\n",
    "\n",
    "schema = inferirSchema(campos_agregados)\n",
    "\n",
    "print schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ahora si, create dataframe\n",
    "user_navigation_statsDF = sqlContext.createDataFrame(allusers_maxsession, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 2) POR VIAJE: Características del viaje del flow más profundo más reciente\n",
    "# \n",
    "\n",
    "# ### Primero me quedo con una acción relevante por userid\n",
    "\n",
    "# - Para los compradores me quedo con la información de viaje del thanks, \n",
    "# - Para los no compradores me quedo con la información del viaje del flow más profundo más reciente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# allusers ya estaba ordenado por datetime\n",
    "\n",
    "def keepRelevantAction(pair):\n",
    "    userid = pair[0]\n",
    "    evs = pair[1]\n",
    "    \n",
    "    evs = list(evs) # convierto a evs, el input de la funcion, en lista, porque la parte Values del RDD es iterable pero no indexable\n",
    "    \n",
    "    for event in evs: \n",
    "        if event['fl'] == 'thanks':\n",
    "            event['flow_depth'] = 4\n",
    "        elif event['fl'] == 'checkout':\n",
    "            event['flow_depth'] = 3\n",
    "        elif event['fl'] == 'detail':\n",
    "            event['flow_depth'] = 2\n",
    "        else:\n",
    "            event['flow_depth'] = 1\n",
    "            \n",
    "    max_flow = max( [ event['flow_depth'] for event in evs] )\n",
    "    max_flow_datetime = max( [ event['datetime'] for event in evs if event['flow_depth'] == max_flow] ) #devolver timestamp mas reciente del flujo mas profundo\n",
    "    \n",
    "    result = [event for event in evs if event['datetime'] == max_flow_datetime]\n",
    "    return result\n",
    "\n",
    "relevant_actions = allusers_con_actiondate.flatMap(keepRelevantAction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ##### Paso esta tabla a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# El problema para pasar de rdd a dataframe con createDataFrame es que inferschema infiere el schema del primer dict, \n",
    "# pero no todos los dict tienen el mismo schema (porque no todos los eventos tienen los mismos campos)\n",
    "\n",
    "#sqlContext.createDataFrame(relevant_actions).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# agrego la variable que cree a los campos que habia pedido\n",
    "# diff, sesion, planning_day_number y local_day_of_year ya no me sirven\n",
    "campos_sin_pr.append(\"action_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hago yo el schema con la funcion que ya tenia\n",
    "schema = inferirSchema(campos_sin_pr)\n",
    "\n",
    "print schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ahora si, create dataframe\n",
    "user_relevant_actionDF = sqlContext.createDataFrame(relevant_actions, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### Uno las dos partes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uno los dos dataframes\n",
    "all_data = user_relevant_actionDF.join(user_navigation_statsDF, user_relevant_actionDF.userid==user_navigation_statsDF.userid, 'outer')        .drop(user_navigation_statsDF[\"userid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #### Agrego GeoData para obtener destination type y horario local de ci y co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importo la data de los iatas de un csv ya calculado. \n",
    "# no funciona poner .schema(customSchema) y aplicar mi propio schema \n",
    "#iata_data = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").load(\"dataset/geoData.csv\")\n",
    "iata_data = sqlContext.read.load(\"dataset/geoData.parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iata_data.registerTempTable(\"iata_data2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iatadata = sqlContext.sql('''\n",
    "SELECT upper(iata) iata_dest, CAST(latitude AS DOUBLE) latitude_dest, CAST(longitude AS DOUBLE) longitude_dest, \n",
    "       upper(country) country_dest, upper(continent) continent_dest, lat_country, lon_country\n",
    "FROM iata_data2 \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hago un merge de los dos dataframes para agregar country, lat, long y continente\n",
    "all_data = all_data.join(iatadata, all_data.dc==iatadata.iata_dest, 'left')        .drop(all_data[\"dc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #### CC Coords \n",
    "# Me quedo con las coordenadas centrales del país del sitio, pidiendo de la tabla de Geo el primer iata de una ciudad en MX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mx_iatas = iata_data.where(iata_data['country'] == site) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc_lat = mx_iatas.select(\"lat_country\").first()[0]\n",
    "cc_lon = mx_iatas.select(\"lon_country\").first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc_lat, cc_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #### Checkpoint time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = checkpoint(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# el count solo se hace para activar el checkpoint\n",
    "all_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### Preparo las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data.registerTempTable(\"all_data2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# en este paso:\n",
    "# - pasé las fechas de ci y co a formato de fecha, \n",
    "# - calculé duración y anticipación\n",
    "# - clasifiqué a los usuarios en couple, single y family\n",
    "# -  clasifiqué los destinos en domestic, latam o rest_of_world\n",
    "# - arreglé las variables de precio que a veces separan decimal con coma y a veces con punto\n",
    "\n",
    "alldata_ant_dur = sqlContext.sql('''\n",
    "SELECT userid, upper(cc) cc, cant_actions actions_count, max_planning_days planning_days_count, max_session session_count, search search_count,\n",
    "    detail detail_count, checkout checkout_count, thanks thanks_count, comprador, \n",
    "    lower(fl) fl,\n",
    "    CAST(action_date AS DATE) action_date,\n",
    "    CAST(ci AS DATE) ci_date,\n",
    "    CAST(co AS DATE) co_date,\n",
    "    DATEDIFF(CAST(co AS DATE), CAST(ci AS DATE)) AS duration,\n",
    "    DATEDIFF(CAST(ci AS DATE), CAST(action_date AS DATE)) AS anticipation, \n",
    "    di,\n",
    "    CASE WHEN di IN (\"2|0|0\", \"2|0\", \"2\") THEN \"couple\"\n",
    "         WHEN di IN (\"1|0|0\", \"1|0\", \"1\") THEN \"single\"\n",
    "                                          ELSE \"family\"\n",
    "         END AS traveler_type,\n",
    "    CASE WHEN cc != country_dest AND continent_dest IN (\"AMC\", \"SA\") THEN \"latam\"\n",
    "         WHEN cc = country_dest THEN \"domestic\"\n",
    "         ELSE \"r_o_w\"\n",
    "         END AS destination_type,\n",
    "    CAST(hc AS INT),\n",
    "    CAST(hr AS INT),\n",
    "    CAST(hid AS INT), \n",
    "    CAST(REGEXP_REPLACE(pritax , ',', '.') AS DOUBLE) pritax,\n",
    "    CAST(REGEXP_REPLACE(pri , ',', '.') AS DOUBLE) pri,\n",
    "    CAST(REGEXP_REPLACE(exch , ',', '.') AS DOUBLE) exch,\n",
    "    upper(cur) cur,\n",
    "    iata_dest, latitude_dest, longitude_dest, country_dest, continent_dest,\n",
    "    xClient\n",
    "FROM all_data2 \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #### Ratio finde\n",
    "# Nro. de viernes, sábados y domingos de estadía, dividido la duración total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define udf\n",
    "#### cuenta como finde las noches de ci viernes,sábado y domingo.\n",
    "def calcRatioFinde(ci, duracion):\n",
    "    \n",
    "    if ci is None or duracion is None:\n",
    "        return None\n",
    "    \n",
    "    dia_ci = ci.isoweekday()\n",
    "    # cuantas semanas completas de 7 dias (con dos dias de finde) hay en la duracion del viaje?\n",
    "    # trunc(duracion/7)\n",
    "    semanas_completas = int(duracion/7)\n",
    "    \n",
    "    # el resto de los dias fuera de las semanas completas:\n",
    "    resto_dias = duracion - semanas_completas * 7\n",
    "   \n",
    "    # cuento los dias de fin de semana extras a la semana completa\n",
    "    if dia_ci == 5: # si el grupo de dias extra empieza un viernes, como maximo pueden contener vie, sab y dom (3 dias) de finde\n",
    "        dia_finde_extra = min(3, resto_dias) \n",
    "    elif dia_ci == 6:  # si el grupo de dias extra empieza un sabado, como maximo pueden contener sab y dom (2 dias) de finde\n",
    "        dia_finde_extra = min(2, resto_dias) \n",
    "    elif dia_ci == 7:  # si el grupo de dias extra empieza un domingo, si son 6 dias extra contienen un domingo un un viernes\n",
    "        if resto_dias == 6:\n",
    "            dia_finde_extra = 2\n",
    "        else:\n",
    "            dia_finde_extra = 1\n",
    "    elif dia_ci + resto_dias == 6: # abarca todos los casos en que el grupo de dias extra  empieza lu/ma/mi/ju y termina en vi\n",
    "        dia_finde_extra = 1\n",
    "    elif dia_ci + resto_dias == 7: # abarca todos los casos en que el grupo de dias extra  empieza lu/ma/mi/ju y termina en sa\n",
    "        dia_finde_extra = 2\n",
    "    elif dia_ci + resto_dias > 8: # abarca los casos en que el grupo de dias extra empieza lu/ma/mi/ju e incluye un dom\n",
    "        dia_finde_extra = 3\n",
    "    else:\n",
    "        dia_finde_extra = 0\n",
    "    \n",
    "    # Saco el numero de dias de finde (dos por semana completa, mas los dias extra)\n",
    "    # el round es porque python, si divide integers, redondea a integer la respuesta\n",
    "    dias_finde_total = semanas_completas * 2 + round(dia_finde_extra,2)\n",
    "    \n",
    "    # saco el ratio \"dias de finde\" / \"duracion\" del viaje\n",
    "    ratio_finde = dias_finde_total / duracion\n",
    "    \n",
    "    return round(ratio_finde, 2)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "udfcalcRatioFinde = udf(calcRatioFinde, DoubleType())\n",
    "\n",
    "alldata_rfinde= alldata_ant_dur.withColumn(\"ratio_finde\", udfcalcRatioFinde(\"ci_date\", \"duration\"))\n",
    "\n",
    "\n",
    "# #### Distancia en km\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# define udf\n",
    "#Calculate the great circle distance between two points on the earth (specified in decimal degrees)\n",
    "def haversine(lon1, lat1, lon2, lat2): \n",
    "    \n",
    "    if lon1 is None or lon2 is None:\n",
    "        return None\n",
    "    \n",
    "    radius = 6371 # Radius of earth in kilometers. Use 3956 for miles    \n",
    "    # convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    #c = 2 * asin(sqrt(a)) \n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    return round(c * radius, 2)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "udfhaversine = udf(haversine, DoubleType())\n",
    "\n",
    "alldata_dist = alldata_rfinde.withColumn(\"distance_km\", udfhaversine(\"longitude_dest\", \"latitude_dest\", lit(cc_lon), lit(cc_lat)))\n",
    "# como la udf toma los parametros de las columnas de mi DF, no puedo darle un valor fijo como input \n",
    "# pero si uso lit() me crea una columna con ese valor constante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PDF['ratio_finde'] = PDF.apply(lambda r: calcRatioFinde(r.ci_date, r.duration), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #### Guardar como Pandas DF ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alldataPDF = alldata_dist.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alldataPDF.to_csv(\"dynamic_fee_processed_user_actions_MX_4WeeksTo18jul.csv\", sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
